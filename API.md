# API

## Frontend-Backend

## Backend-LLM

### http://example.com:5000/generate

POST:

```json
{
    "prompt":"The direct instruction to the LLM"
}
```

Response:

```json
{
    "response":"The LLM's answer to the prompt"
}
```

### http://example.com:5001/rag/database

<!-- 通过知识库生成回复 -->

POST:

```json
{
    "user_question":"The original question raised by a user"
}
```

Response:

```json
{
    "prompt":"A processed question with RAG according to the database"
}
```

### http://example.com:5001/rag/tf

<!-- 判断题生成 -->

POST:

```json
{
    "knowledge_point":"Knowledge points surrounding the TF questions to be generated by the large model",
    "reference":"Some specific examples or related information on how the TF questions are supposed to be generated",
    "number":"How many TF questions are required"
}
```

Response:

```json
{
    "response":"The LLM's answer to the prompt"
}
```

### http://example.com:5001/rag/probe

<!-- 追问模式 -->

POST:

```json
{
    "knowledge_point":"Knowledge points surrounding the sequential questions to be generated by the large model",
    "reference":"Some specific examples or related information on how the sequential questions are supposed to be generated",
    "number":"How many sequential questions are required"
}
```

Response:

```json
{
    "response":"The LLM's answer to the prompt"
}
```

### http://example.com:5001/rag/quiz

<!-- 辅助出卷 -->

POST:

```json
{
    "knowledge_point":"Knowledge points surrounding the quiz to be generated by the large model",
    "reference":"Some specific examples or related information on how the quiz is supposed to be generated",
    "number":"Number of questions in the quiz"
}
```

Response:

```json
{
    "response":"The LLM's answer to the prompt"
}
```